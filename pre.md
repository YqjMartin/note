<font size = 4>

### Conformer: Convolution-augmented Transformer for Speech Recognition

<div align=center>
	<img src=conformer.png>
</div>

Wu et al.[17] proposed a multi-branch architecture with splitting the input into two branches: self-attention and convolution; and concatenating their outputs.

Wuç­‰äººçš„æ–‡ç« ä¸­æ˜¯å°†attentionå’Œconvolutionä½œä¸ºä¸¤ä¸ªåˆ†æ”¯ï¼Œè€Œè¿™ç¯‡æ–‡ç« åˆ™æ˜¯æå‡ºäº†ä¸²è¡Œæ‹¼æ¥çš„æ¶æ„ã€‚

![alt text](cnn_block.png)
ä¼ ç»Ÿå·ç§¯ï¼š![alt text](cnn.jpg)
é€ç‚¹å·ç§¯--PointwiseConvï¼š![alt text](PointwiseConv.png)
æ·±åº¦å·ç§¯--DepthwiseConvï¼š![alt text](DepthwiseConv.png)
æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼ˆDepthwise Separable Convolutionï¼‰<https://arxiv.org/abs/1610.02357>
[ç›¸å…³æ–‡ç« --NEU NLPlab](https://school.niutrans.com/qualityArticleInfo?id=94)

### CONTRASTIVE AUDIO-VISUAL MASKED AUTOENCODER

#### 1 WHY
1. ä»äººç±»è§’åº¦è®¤è¯†ä¸–ç•Œæ˜¯ç»“åˆäº†å¬è§‰å’Œè§†è§‰
2. æ¨¡å‹è¶Šæ¥è¶Šå¤§ï¼Œæ ‡è®°æ•°æ®è¿‡äºæ˜‚è´µ----è‡ªç›‘ç£
3. å¦‚ä½•ç»“åˆä¸¤ç§æ¨¡æ€----å¯¹æ¯”å­¦ä¹ 
4. MAEåœ¨SATå’ŒViTä¸­è¾¾åˆ°sota
5. CAV + MAE ï¼ˆinnovationï¼‰
#### 2 HOW
![alt text](CAV-MAE.png)
1. CAV ï¼šencode + contrastive loss
2. MAEï¼šself-supervised learing &emsp;[ç›¸å…³è§£è¯»](https://zhuanlan.zhihu.com/p/439554945)
3. AV-MAEï¼šåŠ å…¥äº†ä¸€ä¸ªtype embeddingä¹‹åå°±ç›´æ¥concateæ”¾åˆ°ä¸€ä¸ªencoderä¸­ï¼ˆå¯¹ä¸åŒçš„æ¨¡æ€é‡‡ç”¨ç›¸åŒçš„æƒé‡è¿›è¡Œè®­ç»ƒï¼Œæ•ˆæœä¸å¥½ï¼‰
4. CAV-MAEï¼šç”¨å•æ¨¡æ€æµè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼ˆCAVï¼‰ï¼Œç”¨å¤šæ¨¡æ€æµè¿›è¡Œé‡æ„ä»»åŠ¡ï¼ˆAV-MAEï¼‰ï¼Œæœ€åLoss = contrastive loss + k*reconstruction loss

### Mamba: Linear-Time Sequence Modeling with Selective State Spaces
#### motivation:
transformeræ¨¡å‹çš„äºŒæ¬¡å¤æ‚åº¦ï¼š

å…·ä½“æ€ä¹ˆè®¡ç®—å¾—æ¥çš„å‘¢ï¼Ÿ

1. ç¬¬ä¸€æ­¥æ˜¯è®¡ç®—Qâ€‹ã€Kâ€‹ã€Vâ€‹
å³ $Q=x W_{Q}, K=x W_{K}, V=x W_{V}$
è¯¥çŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º $[b, N, d] \times[d, d] \rightarrow[b, N, d]â€‹$
è®¡ç®—é‡ä¸ºï¼š$3 * 2 b N d^{2}=6 b N d^{2}â€‹$
$\rightarrow$  1ï¼Œâ€œ(b,N,d)çœ‹åšbä¸ª(N,d)ï¼Œ(b,N,d) Ã— (d,d)çœ‹åšbä¸ª(N,d) Ã— (d,d)ï¼Œ(N,d) Ã— (d,d)çš„è®¡ç®—æ¬¡æ•°æ˜¯2Ndd(ä¹˜æ³•Nddã€åŠ æ³•å†Nddï¼Œå½“ç„¶ä¹Ÿæœ‰çš„èµ„æ–™ä¸çœ‹åŠ æ³•)bä¸ª(N,d) Ã— (d,d)çš„è®¡ç®—æ¬¡æ•°å°±æ˜¯b2Nddï¼Œä¹Ÿå°±æ˜¯$2bN{d}^2$â€
$\rightarrow$  2ï¼Œxçš„å½¢çŠ¶æ˜¯[b,N,d]ï¼ŒW_{Q}çš„å½¢çŠ¶æ˜¯[d,d],Qçš„å½¢çŠ¶æ˜¯[b,N,d]ï¼Œå› ä¸ºé™¤äº†Qä¹‹å¤–ï¼Œè¿˜å¾—å†è®¡ç®—Kã€Vï¼Œæ‰€ä»¥æœ€åä¼šå†ä¹˜ä»¥ä¸ª3ï¼Œå¾—åˆ°ï¼š$6bNd^{2}$
2. è®¡ç®—$Q K^Tâ€‹$
è¯¥éƒ¨åˆ†çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º
$\left[b, h e a d \_n u m, N, p e r \_h e a d \_h i d d e n \_s i z e\right]â€‹ \timesâ€‹ \left[b, h e a d \_n u m, p e r \_h e a d \_h i d d e n \_s i z e\right. , N]\rightarrow\left[b, h e a d \_n u m, N, N\right]â€‹$
è®¡ç®—é‡ä¸ºï¼š<mark>$2bN^2d$â€‹</mark>
1. è®¡ç®—åœ¨Vâ€‹ä¸Šçš„åŠ æƒ $score \cdot Vâ€‹$
è¯¥éƒ¨åˆ†çŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º
$\left[b, h e a d \_n u m, N, N\right] \times\left[b, h e a d \_n u m, N, p e r \_h e a d \_h i d d e n \_s i z e\right]â€‹ \rightarrow\left[b, h e a d \_n u m, N, p e r \_h e a d \_h i d d e n \_s i z e\right]â€‹$
è®¡ç®—é‡ä¸ºï¼š<mark>$2bN^2dâ€‹$</mark>
1. attentionåçš„çº¿æ€§æ˜ å°„ï¼ŒçŸ©é˜µä¹˜æ³•çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶ä¸º$[b, N, d] \times[d, d] \rightarrow[b, N, d]â€‹$
è®¡ç®—é‡ä¸º$2bNd^2â€‹$
* æœ€ç»ˆè‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç»“æœä¸º
$x_{o u t}=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d}}\right) \cdot V \cdot W_{o}+x$

![attention_complexity](attention_complexity.png)

---

#### model:
- Selective State Space Models
	 - Structured state space sequence models (S4) 
		![s4](s4.png)
		(1a,1b)æ§åˆ¶ç†è®ºä¸­çš„æ–¹ç¨‹ï¼Œ(2a,2b)ç¦»æ•£åŒ–ä¹‹åçš„æ–¹ç¨‹ï¼Œ(3a,3b)å·ç§¯å½¢å¼çš„æ–¹ç¨‹<br>
		å¯¹æ¯”RNNæ–¹ç¨‹ï¼š
		$\begin{aligned}
		& h_t=\tanh(W_{hh}h_{t-1}+W_{xh}x_t) \\
		& y_t=W_{hy}h_t
		\end{aligned}$ 
		<br>

		<details>
		<summary> å·ç§¯å½¢å¼è§£é‡Š </summary>

		![s4_conv](s4_conv.png)
		è‡³äºä¸Šå›¾ä¸­çš„y_2æ˜¯å’‹è®¡ç®—å¾—åˆ°çš„ï¼Œåˆ«å¿˜äº†æˆ‘ä¸Šé¢æ¨å¯¼å‡ºæ¥çš„
		$\begin{aligned} y_{2} & =C h_{2} \\ & =C\left(\bar{A} h_{1}+\bar{B} x_{2}\right) \\ & =C\left(\bar{A}\left({\bar{A} h_{0}+\bar{B} x_{1}}\right)+\bar{B} x_{2}\right) \\ & =C\left(\bar{A}\left(\bar{A} \cdot \bar{B} x_{0}+\bar{B} x_{1}\right)+\bar{B} x_{2}\right) \\ & =C\left(\bar{A} \cdot \bar{A} \cdot \bar{B} x_{0}+\bar{A} \cdot \bar{B} x_{1}+\bar{B} x_{2}\right) \\ & =C \cdot \bar{A}^2 \cdot \bar{B} x_{0}+C \cdot \bar{A} \cdot \bar{B} \cdot x_{1}+C \cdot \bar{B} x_{2} \end{aligned}$
		è‹¥æ¨è€Œå¹¿ä¹‹ï¼Œå¯å¾—
		$y_{k}=C \bar{A}^{k} \bar{B} x_{0}+C \bar{A}^{k-1} \bar{B} x_{1}+\cdots+C \bar{A} \bar{B} x_{k-1}+C \bar{B} x_{k}$

		æ­¤å¤–ï¼Œæ¢ä¸ªå½¢å¼çœ‹ï¼Œæ˜¯ä¸æ„å‘³ç€y_3å®é™…ä¸Šå¯ä»¥è®¡ç®—ä¸ºç‚¹ç§¯ï¼Œå…¶ä¸­å³ä¾§å‘é‡æ˜¯æˆ‘ä»¬çš„è¾“å…¥x
		$y_{3}=\left(\begin{array}{llll} \mathbf{C} \overline{\mathbf{A}} \overline{\mathbf{A}} \overline{\mathbf{A}} \overline{\mathbf{B}} & \mathbf{C} \overline{\mathbf{A}} \overline{\mathbf{A}} \overline{\mathbf{B}} & \mathbf{C} \overline{\mathbf{A}} \overline{\mathbf{B}} & \mathbf{C} \overline{\mathbf{B}} \end{array}\right)\left(\begin{array}{l} x_{0} \\ x_{1} \\ x_{2} \\ x_{3} \end{array}\right)$
		ç”±äºå…¶ä¸­ä¸‰ä¸ªç¦»æ•£å‚æ•°Aã€Bã€Céƒ½æ˜¯å¸¸æ•°ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é¢„å…ˆè®¡ç®—å·¦ä¾§å‘é‡å¹¶å°†å…¶ä¿å­˜ä¸ºå·ç§¯æ ¸ï¼Œè¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§ä½¿ç”¨å·ç§¯è¶…é«˜é€Ÿè®¡ç®—yçš„ç®€å•æ–¹æ³•ï¼Œå¦‚ä»¥ä¸‹ä¸¤ä¸ªæ–¹ç¨‹æ‰€ç¤º
		$\begin{aligned} \overline{\mathbf{K}} & =\left(\begin{array}{llll} \mathbf{C} \overline{\mathbf{B}} & \mathbf{C} \overline{\mathbf{A}} \overline{\mathbf{B}} & \cdots & \mathbf{C A}^{\mathbf{k}} \overline{\mathbf{B}} \end{array}\right) \\ y & =\overline{\mathbf{K}} * x \end{aligned}$
		(åƒæ˜¯ä¸€ä¸ªè¶…å‰è¿›ä½åŠ æ³•å™¨)
		</details>

		![s4_3](s4_3.png)
		å³åœ¨è®­ç»ƒæ—¶ç”¨RNNï¼Œåœ¨æ¨ç†æ—¶ç”¨CNN
		<br>

		ä½¿ç”¨HIPPOçŸ©é˜µæ¥å¢å¼ºRNNè®°å¿†åŠ›
		![hippo](hippo.png)
		![hippo_ad](hippo_ad.png)
		>$\overline A$çŸ©é˜µæ˜¯ç”¨æ¥æ›´æ–° hidden state çš„, è€Œæˆ‘ä»¬çŸ¥é“ RNN è¢«è¯Ÿç—…çš„ä¸€ä¸ªç‚¹æ°æ°æ˜¯ hidden state çš„è®°å¿†èƒ½åŠ›æœ‰é™. æƒ³æƒ³è¿™ä¸ªä¹Ÿå¯ä»¥ç†è§£, å› ä¸º hidden state çš„å¤§å°æ˜¯å›ºå®šçš„, ä½†æ˜¯éœ€è¦è®°å¿†çš„å†…å®¹æ˜¯éšç€ sequence length å¢åŠ çš„, ç”¨ä¸€ä¸ªæœ‰é™çš„å®¹å™¨å»è£…æºæºä¸æ–­çš„æ°´æµ, è‡ªç„¶è¦æœ‰æº¢å‡º.
		Mamba æƒ³è¦æ”¹å–„è¿™ä¸ªé—®é¢˜. å¥¹çš„åšæ³•æ˜¯å¯¹$\overline A$æä¸€äº›æ•°å­¦. å…ˆæ¥å®šä¹‰ä¸€ä¸‹, ä»€ä¹ˆå«åšä¸€ä¸ªå¥½çš„ hidden state çš„è®°å¿†, é‚£ç†æƒ³æƒ…å†µè‚¯å®šå°±æ˜¯, ä»»ä½•æ—¶å€™éƒ½å¯ä»¥ç”¨$h_{l}$æ— æŸæ¢å¤æ‰€æœ‰è§è¿‡çš„ input. è¿™å¤§æ¦‚ç‡ä¸å¯èƒ½, é‚£ä¹ˆæŸå°ç‚¹ä¹Ÿè¡Œ. è¿™æ ·, æˆ‘ä»¬å°±æŠŠ hidden state çš„è®°å¿†èƒ½åŠ›è¡¡é‡æˆäº†å¯ä»¥æœ‰å¤šå°‘æŸå¤±çš„æ¢å¤ input.
		å°±åƒå‰é¢çš„ å¾®åˆ†æ–¹ç¨‹ çš„éƒ¨åˆ†ä¸€æ ·, Mamba ç”¨äº†å¦å¤–ä¸€ç§ç†è®ºæˆæœ HiPPOæ¥æ•°å­¦åœ°æ„å»º$\overline A$çŸ©é˜µ.

	- Selective State Space Models
		![s6](s6.png)
		We specifically choose $ğ‘ _{ğµ} (ğ‘¥)= Linear_{ğ‘} (ğ‘¥), ğ‘ _{ğ¶} (ğ‘¥) = Linear_{ğ‘} (ğ‘¥), ğ‘ _{Î”}(ğ‘¥) = Broadcast_{ğ·} (Linear_{1}(ğ‘¥)), and ğœ_{Î”} = softplus $, where $Linear_{ğ‘‘}$ is a parameterized projection to dimension ğ‘‘. The choice of $ğ‘ _{Î”}$ and $ğœ_{Î”}$ is due to a connection to RNN gating mechanisms explained in Section 3.5.

		![mamba_block](mamba_block.png)
		>ä¸ºä»€ä¹ˆSSMå‰é¢æœ‰ä¸ªå·ç§¯ï¼Ÿ
		æœ¬è´¨æ˜¯å¯¹æ•°æ®åšè¿›ä¸€æ­¥çš„é¢„å¤„ç†ï¼Œæ›´ç»†èŠ‚çš„åŸå› åœ¨äºï¼š
		$\rightarrow$  SSMä¹‹å‰çš„CNNè´Ÿè´£æå–å±€éƒ¨ç‰¹å¾(å› å…¶æ“…é•¿æ•æ‰å±€éƒ¨çš„çŸ­è·ç¦»ç‰¹å¾)ï¼Œè€ŒSSMåˆ™è´Ÿè´£å¤„ç†è¿™äº›ç‰¹å¾å¹¶æ•æ‰åºåˆ—æ•°æ®ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œä¸¤è€…ç®—äº’ä¸ºè¡¥å……
		$\rightarrow$  CNNæœ‰åŠ©äºå»ºç«‹tokenä¹‹é—´çš„å±€éƒ¨ä¸Šä¸‹æ–‡å…³ç³»ï¼Œä»è€Œé˜²æ­¢ç‹¬ç«‹çš„tokenè®¡ç®—
		æ¯•ç«Ÿå¦‚æœæ¯ä¸ª token ç‹¬ç«‹è®¡ç®—ï¼Œé‚£ä¹ˆæ¨¡å‹å°±ä¼šä¸¢å¤±åºåˆ—ä¸­ token ä¹‹é—´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡å…ˆè¿›è¡Œå·ç§¯æ“ä½œï¼Œå¯ä»¥ç¡®ä¿åœ¨è¿›å…¥ SSM ä¹‹å‰ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ª token å·²ç»è€ƒè™‘äº†å…¶é‚»å±… token çš„ä¿¡æ¯ã€‚è¿™æ ·ï¼Œæ¨¡å‹å°±ä¸ä¼šå•ç‹¬åœ°å¤„ç†æ¯ä¸ª tokenï¼Œè€Œæ˜¯åœ¨å¤„ç†æ—¶è€ƒè™‘äº†æ•´ä¸ªå±€éƒ¨ä¸Šä¸‹æ–‡

- Hardware-aware Algorithm
  - parallel scan
	![parallel_scan](parallel_scan.png)
	ä¹Ÿå°±æ˜¯ç±»ä¼¼äºä¸€ä¸ªè¶…å‰è¿›ä½åŠ æ³•å™¨
  - Flash Attention
  	![Hardware-aware](Hardware-aware.png)

ç›¸å…³è¿æ¥ï¼š
[çŸ¥ä¹](https://www.zhihu.com/question/644981978/answer/3405813530?utm_oi=922122523653582848&utm_psn=1749458306760822784)    
[CSDN](https://blog.csdn.net/v_JULY_v/article/details/134923301)
[flash attention](https://arxiv.org/abs/2205.14135)
[linear attention](https://arxiv.org/abs/2007.14902)







  



